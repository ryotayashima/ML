{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"genText_nietzsche_LSTM.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"8oZ3k1u_Mp-o","colab_type":"code","colab":{}},"cell_type":"code","source":["!mkdir log_dir"],"execution_count":0,"outputs":[]},{"metadata":{"id":"J0B5RBvj-UUn","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install optuna"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CxdIxy8eIl9O","colab_type":"code","outputId":"cccf56a2-0f38-4dcb-ea33-4f9b7e328213","executionInfo":{"status":"ok","timestamp":1556158810126,"user_tz":-540,"elapsed":4111,"user":{"displayName":"トムー","photoUrl":"","userId":"01084465765645554068"}},"colab":{"base_uri":"https://localhost:8080/","height":91}},"cell_type":"code","source":["import keras\n","from keras import layers\n","import numpy as np\n","import random\n","import sys\n","import optuna\n","\n","'''\n","サンプル(コーパス)のダウンロード\n","'''\n","path = keras.utils.get_file(\n","    \"nietzsche.txt\",\n","    origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\n",")\n","text = open(path).read().lower()\n","print(\"Corpus length:\", len(text))\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n","606208/600901 [==============================] - 0s 0us/step\n","Corpus length: 600893\n"],"name":"stdout"}]},{"metadata":{"id":"gNIAcsFEI7us","colab_type":"code","outputId":"1e968d7d-c9ab-4c1d-da53-688f0b297f60","executionInfo":{"status":"ok","timestamp":1556158817622,"user_tz":-540,"elapsed":4826,"user":{"displayName":"トムー","photoUrl":"","userId":"01084465765645554068"}},"colab":{"base_uri":"https://localhost:8080/","height":73}},"cell_type":"code","source":["'''\n","文字シーケンスのベクトル化\n","'''\n","maxlen = 60         # 60文字のシーケンスを抽出\n","step = 3            # 3文字おきに新しいシーケンスをサンプリング\n","sentences = []      # 抽出されたシーケンスを保持\n","next_chars = []     # 目的値(次に来る文字)を保持\n","\n","for i in range(0, len(text) - maxlen, step):\n","    sentences.append(text[i:i+maxlen])\n","    next_chars.append(text[i+maxlen])\n","\n","print(\"Number of sequences:\", len(sentences))\n","\n","# コーパスの一意な文字のリスト\n","chars = sorted(list(set(text)))         # set()->重複する要素を削除, list()->リスト型にキャスト\n","print(\"Unique character:\", len(chars))\n","\n","# これらの文字をリストcharsのインデックスにマッピングするディクショナリ\n","char_indicies = dict((char, chars.index(char)) for char in chars)\n","\n","print(\"Vectorization...\")\n","\n","# one-hotコーディングを適用して文字を2値の配列に格納\n","x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n","y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n","for i, sentence in enumerate(sentences):\n","    for t, char in enumerate(sentence):\n","        x[i, t, char_indicies[char]] = 1\n","    y[i, char_indicies[next_chars[i]]] = 1\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Number of sequences: 200278\n","Unique character: 57\n","Vectorization...\n"],"name":"stdout"}]},{"metadata":{"id":"5ssXtti-I_3s","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","モデルの構築\n","'''\n","callbacks_list = [\n","    keras.callbacks.ModelCheckpoint(\n","        filepath=\"my_model.h5\",     # モデルの保存先となるファイルのパス\n","        monitor='loss',         # lossを監視\n","        save_best_only=True       # lossが改善した時のみ保存\n","    ),\n","    keras.callbacks.TensorBoard(\n","        log_dir=\"log_dir\",            # ログの記録場所\n","        histogram_freq=0,            # 1エポックごとに活性化ヒストグラムを記録\n","        write_images = True\n","    )\n","]\n","\n","model = keras.models.Sequential()\n","model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n","model.add(layers.Dense(len(chars), activation=\"softmax\"))\n","optimizer = keras.optimizers.RMSprop(lr=0.01)\n","model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n","\n","def sample(preds, temperature=1.0):\n","    preds = np.asarray(preds).astype(\"float64\")\n","    preds = np.log(preds) / temperature\n","    exp_preds = np.exp(preds)\n","    preds = exp_preds / np.sum(exp_preds)\n","    probabs = np.random.multinomial(1, preds, 1)\n","    return np.argmax(probabs)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"i710l01tJDcs","colab_type":"code","outputId":"c07e5012-a314-4b50-b403-910746cd9624","executionInfo":{"status":"ok","timestamp":1556162404174,"user_tz":-540,"elapsed":640298,"user":{"displayName":"トムー","photoUrl":"","userId":"01084465765645554068"}},"colab":{"base_uri":"https://localhost:8080/","height":2583}},"cell_type":"code","source":["'''\n","言語モデルの訓練とサンプリング\n","'''\n","# import keras.backend as K\n","# K.clear_session()\n","# 1エポックでデータを学習\n","model.fit(\n","    x, y,\n","    batch_size=2048,\n","    epochs=60,\n","    callbacks=callbacks_list,\n","    validation_split = 0.3\n"," )\n","\n","# テキストシートをランダムに選択\n","start_index = random.randint(0, len(text) - maxlen -1)\n","generated_text = text[start_index: start_index + maxlen]\n","print('---Generating with seed: \"'+ generated_text +'\"')\n","\n","# ある範囲内のサンプリング温度を試してみる\n","for temperature in [0.5, 1.0]:\n","    print(\"------temperature:\", temperature)\n","    print(\"\")\n","    sys.stdout.write(generated_text)\n","    print(\"\")\n","\n","    # 400文字を生成\n","    for i in range(400):\n","       # これまでに生成された文字にone-hotコーディングを適用\n","        sampled = np.zeros((1, maxlen, len(chars)))\n","        for t, char in enumerate(generated_text):\n","           sampled[0, t, char_indicies[char]] = 1.\n","\n","            # 次の文字をサンプリング\n","        preds = model.predict(sampled, verbose=0)[0]\n","        next_index = sample(preds, temperature)\n","        next_char = chars[next_index]\n","\n","        generated_text += next_char\n","        generated_text = generated_text[1:]\n","\n","        sys.stdout.write(next_char)\n","        sys.stdout.flush()\n"],"execution_count":51,"outputs":[{"output_type":"stream","text":["Train on 140194 samples, validate on 60084 samples\n","Epoch 1/60\n","140194/140194 [==============================] - 11s 80us/step - loss: 2.8206 - val_loss: 2.4182\n","Epoch 2/60\n","140194/140194 [==============================] - 10s 72us/step - loss: 2.2526 - val_loss: 2.1432\n","Epoch 3/60\n","140194/140194 [==============================] - 10s 75us/step - loss: 2.0082 - val_loss: 1.9841\n","Epoch 4/60\n","140194/140194 [==============================] - 10s 74us/step - loss: 1.8612 - val_loss: 1.8797\n","Epoch 5/60\n","140194/140194 [==============================] - 10s 73us/step - loss: 1.7531 - val_loss: 1.8029\n","Epoch 6/60\n","140194/140194 [==============================] - 10s 72us/step - loss: 1.6720 - val_loss: 1.7357\n","Epoch 7/60\n","140194/140194 [==============================] - 10s 72us/step - loss: 1.6087 - val_loss: 1.7002\n","Epoch 8/60\n","140194/140194 [==============================] - 11s 76us/step - loss: 1.5549 - val_loss: 1.7002\n","Epoch 9/60\n","140194/140194 [==============================] - 10s 75us/step - loss: 1.5120 - val_loss: 1.6721\n","Epoch 10/60\n","140194/140194 [==============================] - 10s 73us/step - loss: 1.4754 - val_loss: 1.6700\n","Epoch 11/60\n","140194/140194 [==============================] - 10s 72us/step - loss: 1.4426 - val_loss: 1.6708\n","Epoch 12/60\n","140194/140194 [==============================] - 10s 72us/step - loss: 1.4133 - val_loss: 1.6742\n","Epoch 13/60\n","140194/140194 [==============================] - 10s 73us/step - loss: 1.3921 - val_loss: 1.6484\n","Epoch 14/60\n","140194/140194 [==============================] - 11s 79us/step - loss: 1.3682 - val_loss: 1.6661\n","Epoch 15/60\n","140194/140194 [==============================] - 10s 72us/step - loss: 1.3465 - val_loss: 1.6536\n","Epoch 16/60\n","140194/140194 [==============================] - 11s 79us/step - loss: 1.3266 - val_loss: 1.6732\n","Epoch 17/60\n","140194/140194 [==============================] - 10s 73us/step - loss: 1.3113 - val_loss: 1.6775\n","Epoch 18/60\n","140194/140194 [==============================] - 10s 72us/step - loss: 1.2934 - val_loss: 1.6888\n","Epoch 19/60\n","140194/140194 [==============================] - 10s 73us/step - loss: 1.2779 - val_loss: 1.6862\n","Epoch 20/60\n","140194/140194 [==============================] - 10s 72us/step - loss: 1.3136 - val_loss: 1.6873\n","Epoch 21/60\n","140194/140194 [==============================] - 10s 71us/step - loss: 1.2523 - val_loss: 1.7061\n","Epoch 22/60\n","140194/140194 [==============================] - 10s 71us/step - loss: 1.2418 - val_loss: 1.7215\n","Epoch 23/60\n","140194/140194 [==============================] - 10s 71us/step - loss: 1.2347 - val_loss: 1.7146\n","Epoch 24/60\n","140194/140194 [==============================] - 11s 78us/step - loss: 1.2246 - val_loss: 1.7218\n","Epoch 25/60\n","140194/140194 [==============================] - 10s 71us/step - loss: 1.2204 - val_loss: 1.7426\n","Epoch 26/60\n","140194/140194 [==============================] - 10s 71us/step - loss: 1.2043 - val_loss: 1.7367\n","Epoch 27/60\n","140194/140194 [==============================] - 10s 70us/step - loss: 1.1970 - val_loss: 1.7516\n","Epoch 28/60\n","140194/140194 [==============================] - 10s 70us/step - loss: 1.1875 - val_loss: 1.7556\n","Epoch 29/60\n","140194/140194 [==============================] - 10s 70us/step - loss: 1.1807 - val_loss: 1.7938\n","Epoch 30/60\n","140194/140194 [==============================] - 10s 70us/step - loss: 1.1730 - val_loss: 1.7615\n","Epoch 31/60\n","140194/140194 [==============================] - 10s 70us/step - loss: 1.1597 - val_loss: 1.7801\n","Epoch 32/60\n","140194/140194 [==============================] - 11s 78us/step - loss: 1.1615 - val_loss: 1.7887\n","Epoch 33/60\n","140194/140194 [==============================] - 10s 75us/step - loss: 1.1568 - val_loss: 1.7760\n","Epoch 34/60\n","140194/140194 [==============================] - 10s 71us/step - loss: 1.1520 - val_loss: 1.7905\n","Epoch 35/60\n","140194/140194 [==============================] - 10s 71us/step - loss: 1.1456 - val_loss: 1.7973\n","Epoch 36/60\n","140194/140194 [==============================] - 10s 70us/step - loss: 1.1408 - val_loss: 1.8070\n","Epoch 37/60\n","140194/140194 [==============================] - 10s 71us/step - loss: 1.1359 - val_loss: 1.8180\n","Epoch 38/60\n","140194/140194 [==============================] - 10s 71us/step - loss: 1.1316 - val_loss: 1.8223\n","Epoch 39/60\n","140194/140194 [==============================] - 10s 71us/step - loss: 1.1283 - val_loss: 1.8174\n","Epoch 40/60\n","140194/140194 [==============================] - 11s 77us/step - loss: 1.1240 - val_loss: 1.8375\n","Epoch 41/60\n","140194/140194 [==============================] - 10s 71us/step - loss: 1.1209 - val_loss: 1.8257\n","Epoch 42/60\n","140194/140194 [==============================] - 10s 70us/step - loss: 1.1155 - val_loss: 1.8439\n","Epoch 43/60\n","140194/140194 [==============================] - 10s 71us/step - loss: 1.1138 - val_loss: 1.8537\n","Epoch 44/60\n","140194/140194 [==============================] - 11s 76us/step - loss: 1.1107 - val_loss: 1.8625\n","Epoch 45/60\n","140194/140194 [==============================] - 10s 74us/step - loss: 1.0998 - val_loss: 1.8691\n","Epoch 46/60\n","140194/140194 [==============================] - 10s 70us/step - loss: 1.1032 - val_loss: 1.8490\n","Epoch 47/60\n","140194/140194 [==============================] - 10s 72us/step - loss: 1.1040 - val_loss: 1.8556\n","Epoch 48/60\n","140194/140194 [==============================] - 11s 76us/step - loss: 1.1002 - val_loss: 1.8634\n","Epoch 49/60\n","140194/140194 [==============================] - 10s 70us/step - loss: 1.0977 - val_loss: 1.8846\n","Epoch 50/60\n","140194/140194 [==============================] - 10s 70us/step - loss: 1.0946 - val_loss: 1.8623\n","Epoch 51/60\n","140194/140194 [==============================] - 10s 70us/step - loss: 1.0910 - val_loss: 1.8757\n","Epoch 52/60\n","140194/140194 [==============================] - 10s 70us/step - loss: 1.0913 - val_loss: 1.8711\n","Epoch 53/60\n","140194/140194 [==============================] - 10s 70us/step - loss: 1.0880 - val_loss: 1.8777\n","Epoch 54/60\n","140194/140194 [==============================] - 10s 70us/step - loss: 1.0844 - val_loss: 1.8971\n","Epoch 55/60\n","140194/140194 [==============================] - 10s 71us/step - loss: 1.0829 - val_loss: 1.8990\n","Epoch 56/60\n","140194/140194 [==============================] - 11s 77us/step - loss: 1.0817 - val_loss: 1.8738\n","Epoch 57/60\n","140194/140194 [==============================] - 10s 70us/step - loss: 1.0789 - val_loss: 1.8875\n","Epoch 58/60\n","140194/140194 [==============================] - 10s 70us/step - loss: 1.0794 - val_loss: 1.8748\n","Epoch 59/60\n","140194/140194 [==============================] - 10s 70us/step - loss: 1.0730 - val_loss: 1.9018\n","Epoch 60/60\n","140194/140194 [==============================] - 10s 70us/step - loss: 1.0724 - val_loss: 1.8977\n","---Generating with seed: \"ort which religion and like mythological monstrosities\n","are c\"\n","------temperature: 0.5\n","\n","ort which religion and like mythological monstrosities\n","are c\n","ommonplaces also of the spirit when in what stroked the good operative the matters: something\n","it more into his permination to him alther all the lovers:--and there is what\n","is the most sufficient decided yeal and self-conditary with the way thing the same all the spirit without itself, and the most believe the spirit to case and loves of soristination, and there is not and more in actood ond rank a------temperature: 1.0\n","\n","oristination, and there is not and more in actood ond rank a\n","nd called the spenially understand it is trourds or \"pured, whichere as the motience-spersionalization of nakion, dring embidife, do\n","spirit we propection. what\n","from themselves? (nature! that on the which it definises, mang frovie truthing is equality--such as it differently, as more from who must be\n","under-as if all-amis more through their discives, the same\n","syn ellew on the same society, which is "],"name":"stdout"}]}]}